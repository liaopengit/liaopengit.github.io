---
layout:     post
title:      爬虫
subtitle:   爬虫进阶
date:       2019-04-11
author:     LP
header-img: img/timg.jpeg
catalog: true
tags:
    - 爬虫
    - python
---
## 引言

    在爬取数据过程中，遇到反爬是必然的，本文将介绍集中反爬的措施。

## 淘宝商品图片爬虫-1


```python
#淘宝商品图片爬虫
#并且添加异常处理
import urllib.request
import re
keyname="excel"
key=urllib.request.quote(keyname)
for i in range(1,3):
    try:
        print("--------正在爬第"+str(i)+"页------------")
        url="https://s.taobao.com/search?q="+key+"&s="+str((i-1)*44)
        data=urllib.request.urlopen(url).read().decode("utf-8","ignore")
        pat='"pic_url":"//(.*?)"'
        imglist=re.compile(pat).findall(data)
        for j in range(0,len(imglist)):
            try:
                thisimg=imglist[j]
                thisimgurl="http://"+thisimg
                localfile="/Users/liaopeng/spyderworksapce/淘宝图片/"+str(i)+"_"+str(j)+".jpg"
                urllib.request.urlretrieve(thisimgurl,filename=localfile)
            except Exception as err:
                pass
    except Exception as err:
        pass
```

## 淘宝商品图片爬虫-2


```python
#淘宝商品图片爬虫
#加入浏览器伪装
import urllib.request
import re
import random
keyname="excel"
key=urllib.request.quote(keyname)
uapools=[
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.79 Safari/537.36 Edge/14.14393",
    "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.22 Safari/537.36 SE 2.X MetaSr 1.0",
    "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Maxthon 2.0)",
    ]

def ua(uapools):
    thisua=random.choice(uapools)
    print(thisua)
    headers=("User-Agent",thisua)
    opener=urllib.request.build_opener()
    opener.addheaders=[headers]
    #安装为全局
    urllib.request.install_opener(opener)
    
for i in range(1,101):
    try:
        print("--------正在爬第"+str(i)+"页------------")
        url="https://s.taobao.com/search?q="+key+"&s="+str((i-1)*44)
        ua(uapools)
        data=urllib.request.urlopen(url).read().decode("utf-8","ignore")
        pat='"pic_url":"//(.*?)"'
        imglist=re.compile(pat).findall(data)
        for j in range(0,len(imglist)):
            try:
                thisimg=imglist[j]
                thisimgurl="http://"+thisimg
                localfile="/Users/liaopeng/spyderworksapce/图片数据/"+str(i)+"_"+str(j)+".jpg"
                urllib.request.urlretrieve(thisimgurl,filename=localfile)
            except Exception as err:
                pass
    except Exception as err:
        pass
    
```

## 反爬虫-IP代理池构建-1


```python
#IP代理池构建的第一种方案(适合于代理IP稳定的情况)
import random
import urllib.request
ippools=[
    "114.230.216.106:31912",
    "49.85.12.116:30348",
    "118.255.255.201:808",
    "127.0.0.1:8888",
    ]#代理可能失效
def ip(ippools):
    thisip=random.choice(ippools)
    print(thisip)
    proxy=urllib.request.ProxyHandler({"http":thisip})
    opener=urllib.request.build_opener(proxy,urllib.request.HTTPHandler)
    urllib.request.install_opener(opener)

for i in range(0,5):
    try:
        ip(ippools)
        url="http://www.baidu.com/"
        data1=urllib.request.urlopen(url).read()
        data=data1.decode("utf-8","ignore")
        print(len(data))
        fh=open("/Users/liaopeng/spyderworksapce/"+str(i)+".html","wb")
        fh.write(data1)
        fh.close()
    except Exception as err:
        print(err)
```

## 反爬虫-IP代理池构建-2(推荐)


```python
#IP代理池实现的第二种方式（接口调用法，这种方法更适合于代理IP不稳定的情况）

import time
import urllib.request
def use_ip(ippools,myurl,thisapi):# ippools标志位 myurl爬取网址 thisapi切换接口
    def api(thisapi): 
        import urllib.request
        print("这一次调用了接口")
        import urllib.request
        urllib.request.urlcleanup()
        thisall=urllib.request.urlopen(thisapi).read().decode("utf-8","ignore")
        print("接口调用完成")
        return thisall
    def ip(ippools):# 将得到的ip真正添加到URL中
        thisip=ippools
        print("当前用的IP是："+ippools)
        proxy=urllib.request.ProxyHandler({"http":thisip})
        opener=urllib.request.build_opener(proxy,urllib.request.HTTPHandler)
        urllib.request.install_opener(opener) #安装为全局
    if(ippools==0):
        ippools=api(thisapi)
        print("提取IP完成")
    ip(ippools)
    url=myurl
    data1=urllib.request.urlopen(url).read()
    data=data1.decode("gbk","ignore")
    return ippools,data

x=0
#登陆注册迅代理 生成API接口 根据生成的接口进行相应切换
thisapi='http://api.xdaili.cn/xdaili-api//privateProxy/getDynamicIP/DD2018441370oHHXnN/7331fb7c2eac11e8bcaf7cd30abda612?returnType=1'
for i in range(0,35):
    try:
        url="http://www.baidu.com"
        if(i%7==0 and i==0):#每爬7次切换ip
            ippools,thispagedata=use_ip(0,url,thisapi)
        elif(i%7==0):
            print("正在延时中…")
            time.sleep(15)
            print("延时完成，正在调取IP")
            ippools,thispagedata=use_ip(0,url,thisapi)
            print("IP调取完成")
        else:
            ippools,thispagedata=use_ip(ippools,url,thisapi)
        print(len(thispagedata))
        x+=1
    except Exception as err:
        print(err)
        x+=1

```

## 反爬虫-IP代理池构建-3


```python
# 使用自建服务器代理IP采集数据
# 比较费时费力
import urllib.request
import re
import time
import weip
mysql_inform={"host":"111.67.194.195","user":"mytestjc","passwd":"weijc7789","port":3306,"db":"mytestjc"}
randid="ip0954"
key="周易风水"
key=urllib.request.quote(key)
name="微信爬虫_"+str(key)
def useproxy(ip_inform):
    ip_host=ip_inform[0]
    ip_port=ip_inform[1]
    proxy=urllib.request.ProxyHandler({"http":ip_host+":"+ip_port})
    opener=urllib.request.build_opener(proxy,urllib.request.HTTPHandler)
    urllib.request.install_opener(opener)
for i in range(0,100):
    ip_inform=weip.getip(mysql_inform,randid)
    useproxy(ip_inform)
    thispageurl="http://weixin.sogou.com/weixin?oq=&query="+key+"&type=2&page="+str(i+1)+"&ie=utf8"
    thispagedata=urllib.request.urlopen(thispageurl).read().decode("utf-8","ignore")
    print(thispageurl)
    print(len(thispagedata))
    pat1='<div class="txt-box">.*?href="(.*?)"'
    rst1=re.compile(pat1,re.S).findall(thispagedata)
    if(len(rst1)==0):
        print("这一页没有爬成功")
        continue
    for j in range(0,len(rst1)):
        thisurl=rst1[j]
        pat2='amp;'
        thisurl=thisurl.replace(pat2,"")
        print(thisurl)
        try:
            urllib.request.urlretrieve(thisurl,filename="/Users/liaopeng/spyderworksapce/微信文章数据/"+str(i)+str(j)+".html")
        except Exception as err:
            pass

```

## 反爬虫-IP代理池构建-4


```python
#如何同时使用用户代理池和IP代理池
import time
def ua_ip(ippools,myurl,thisapi):
    import random
    uapools=[
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.79 Safari/537.36 Edge/14.14393",
        "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.22 Safari/537.36 SE 2.X MetaSr 1.0",
        "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Maxthon 2.0)",
        ]
    import urllib.request
    def api(thisapi):
        print("这一次调用了接口")
        import urllib.request
        urllib.request.urlcleanup()
        thisall=urllib.request.urlopen(thisapi).read().decode("utf-8","ignore")
        print("接口调用完成")
        return thisall
    def ip(ippools,uapools):
        thisua=random.choice(uapools)
        print(thisua)
        headers=("User-Agent",thisua)
        thisip=ippools
        #thisip="127.0.0.1:8888"
        print("当前用的IP是："+thisip)
        proxy=urllib.request.ProxyHandler({"http":thisip})
        opener=urllib.request.build_opener(proxy,urllib.request.HTTPHandler)
        opener.addheaders=[headers]
        urllib.request.install_opener(opener)
    if(ippools==0):
        while True:
            ippools=api(thisapi)
            print("提取IP完成")
            ip(ippools,uapools)
            print("正在验证IP有效性")
            data1=urllib.request.urlopen("http://www.baidu.com").read().decode("utf-8","ignore")
            if(len(data1)>5000):
                print("-----当前IP有效-----")
                break
            else:
                print("-----当前IP无效，正在延时60秒重新切换-----")
                time.sleep(15)
    else:
        ip(ippools,uapools)
    url=myurl
    data1=urllib.request.urlopen(url).read()
    data=data1.decode("gbk","ignore")
    return ippools,data
x=0
thisapi='http://api.xdaili.cn/xdaili-api//privateProxy/getDynamicIP/DD2018441370oHHXnN/7331fb7c2eac11e8bcaf7cd30abda612?returnType=1'
for i in range(0,35):
    try:
        url="http://www.hellobi.com"
        if(i%7==0 and i==0):
            ippools,thispagedata=ua_ip(0,url,thisapi)
        elif(i%7==0):
            print("正在延时中…")
            time.sleep(15)
            print("延时完成，正在调取IP")
            ippools,thispagedata=ua_ip(0,url,thisapi)
            print("IP调取完成")
        else:
            ippools,thispagedata=ua_ip(ippools,url,thisapi)
        print(len(thispagedata))
        x+=1
    except Exception as err:
        print(err)
        x+=1
```
