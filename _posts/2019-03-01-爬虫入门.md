---
layout:     post
title:      爬虫
subtitle:   爬虫入门
date:       2019-03-01
author:     LP
header-img: img/timg.jpeg
catalog: true
tags:
    - 爬虫
    - python
---

## 引言

    数据获取是一切分析的前提，因此爬虫的学习还是很有必要的，本次简要介绍一些爬虫的基础知识。 

## 使用urllib模块进行简单网页爬取

### urlopen/urlretrive/Request


```python
import urllib.request
#爬取到内存中-方法1
data1 =  urllib.request.urlopen("http://www.baidu.com").read().decode('utf-8','ignore')
print(len(data1))

#爬取到内存中-方法2 request封装
url = "http://www.baidu.com"
reg = urllib.request.Request(url)
data2= urllib.request.urlopen(reg).read().decode('utf-8','ignore')
print(len(data2))

#爬取到硬盘中
urllib.request.urlretrieve(url,filename="/Users/liaopeng/spyderworksapce/baidu.html")
```

    153319
    153115





    ('/Users/liaopeng/spyderworksapce/baidu.html',
     <http.client.HTTPMessage at 0x118db9e80>)



### 网页状态码


```python
#获取状态码
file =  urllib.request.urlopen("http://www.baidu.com")
print(file.getcode())
```

    200


## 百度信息自动搜索爬虫实战


```python
#需求 先进行网页分析 在txt文件进行分析
import urllib.request

url = "https://www.baidu.com/s?wd="

key = "scrapy"
#对关键词进行编码 因为url中需要对中文进行处理

key_code = urllib.request.quote(key)
#待检索关键词的url
print(key_code)

url_key = url+key_code+"&ie=utf-8"
print(url_key)

#通过for循环爬取各页面信息 这里连续爬取1到10页
for i in range(0,1):
    print("正在爬取"+str(i+1)+"页数据")
    #根据对刚刚总结的url规律构造当前url
    this_url = url_key+"&pn="+str(i*10)
    print(this_url)
    #爬取这一页数据
    data = urllib.request.urlopen(this_url).read().decode('utf-8','ignore')
    print(type(data))
    #成功得到数据
    #根据正则表达式将爬取到的网页列表中各网页标题进行提取
    import re
    pat = '"title":"(.*?)"'
    rst = re.compile(pat,re.S).findall(data)
    for j in range(0,len(rst)):
        print("第"+str(j)+"条网页标题是:"+str(rst[j]))
        print("--------")
```

    scrapy
    https://www.baidu.com/s?wd=scrapy&ie=utf-8
    正在爬取1页数据
    https://www.baidu.com/s?wd=scrapy&ie=utf-8&pn=0
    <class 'str'>
    第0条网页标题是:Scrapy | A Fast and Powerful Scraping and Web Crawling ...
    --------
    第1条网页标题是:Scrapy入门教程 — Scrapy 0.24.6 文档
    --------
    第2条网页标题是:Scrapy简单入门及实例讲解 - 孔扎根 - 博客园
    --------
    第3条网页标题是:Scrapy 1.6 documentation — Scrapy 1.6.0 documentation
    --------
    第4条网页标题是:Scrapy - 第一个爬虫和我的博客 - 简书
    --------
    第5条网页标题是:Scrapy 中文网
    --------
    第6条网页标题是:Scrapy 0.24 文档 — Scrapy 0.24.6 文档
    --------
    第7条网页标题是:Scrapy爬取美女图片 (原创) - 七夜的故事 - 博客园
    --------
    第8条网页标题是:Scrapy 入门教程 | 菜鸟教程
    --------


## 自动POST请求实战


```python
#自动post爬虫
import urllib.request
import urllib.parse

url = "http://www.iqianyue.com/mypost/"
 
posdata = urllib.parse.urlencode({
    "name":"ceo@iqianyue.com",
    "pass":"aA123456",  
}).encode('utf-8') #将数据使用urlencode编码处理后 使用encode()设置为utf-8编码

req = urllib.request.Request(url,posdata)
req.add_header('User-Agent','Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/53')
data = urllib.request.urlopen(req).read()
print(data)
fhandle  = open("Users/liaopeng/spyderworksapce/post.html","wb")
fhandle.write(data)
fhandle.close()
```

## Cookie处理实战

### cookie的保存和处理


```python
#cookie保存
import urllib.request
import http.cookiejar

#建立cookie处理
cjar = http.cookiejar.CookieJar()
opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cjar))
urllib.request.install_opener(opener)

#cookie读取
print(str(cjar))
```

    <CookieJar[]>


## 浏览器伪装技术实战

### 通过Opener添加headers/通过Request添加headers/批量添加headers


```python
#方法1
import urllib.request
url = "https://www.qiushibaike.com/"
#头文件格式header =（'User-Agent'，具体用户代理）
headers = ('User-Agent','Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36')
opener = urllib.request.build_opener()
opener.addheaders =  [headers]
data = opener.open(url).read()
urllib.request.install_opener(opener)
data = urllib.request.urlopen(url).read() 

#方法2
import urllib.request
url = "https://www.qiushibaike.com/"
headers = {'User-Agent','Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36'),}
opener = urllib.request.build_opener()
for key,values in headers.items():
    item = (key,value)
    headall.append(item)
opener.addheaders = headall
urllib.request.install_opener(opener)
data = urllib.request.urlopen(url).read() 

#方法3
import urllib.request
url = "https://www.qiushibaike.com/"
req0 = urllib.request.Request(url)
req0.add_header('User-Agent','Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36')
req0data=urllib.request.urlopen(req0).read().decode("utf-8","ignore") 
```

## 数据自动写入数据库实战


```python
import pymysql
conn = pymysql.connect(host = "127.0.0.1",user ="root",passwd= "lp650911",db = "ts" )
import urllib.request
import re
for i in range(0,100):
    this_url = "https://edu.hellobi.com/course/"+str(i+1)
    data = urllib.request.urlopen(this_url).read().decode('utf-8','ignore')
    title_pat = '<li class="active">(.*?)</li>' 
    teacher_pat = 'class="lec-name">(.*?)<'
    price_pat = 'div class="course-price">.*?<sub>￥<sub>(.*?)</span>'
    
    title = re.compile(title_pat,re.S).findall(data)
    if(len(title)>0):
        title = title[0]
    else:
        continue
        
    teacher = re.compile(teacher_pat,re.S).findall(data)
    if(len(teacher)>0):
        teacher = teacher[0]
    else:
        teacher = "缺失"
        
    price = re.compile(price_pat,re.S).findall(data)
    if(len(price)>0):
        price = price[0]
    else:
        price = "免费"
    
    #print(title)
    #print(teacher)
    #print(price)
    #print("------------")
    conn.query("INSERT INTO ts_spyder(title,teacher,price) VALUES('"+str(title)+"','"+str(teacher)+"','"+str(price)+"')")
    conn.commit()
```

## 糗事百科网络爬虫项目实战


```python
import urllib.request
import re
import random
import time
uapools=[
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.79 Safari/537.36 Edge/14.14393",
    "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.22 Safari/537.36 SE 2.X MetaSr 1.0",
    "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Maxthon 2.0)",
    ]
def UA():
    opener=urllib.request.build_opener()
    thisua=random.choice(uapools)
    ua=("User-Agent",thisua)
    opener.addheaders=[ua]
    urllib.request.install_opener(opener)
    print("当前使用UA："+str(thisua))
for i in range(0,35):
    UA()
    #thisurl="http://www.qiushibaike.com/8hr/page/"+str(i+1)+"/?s=4948859"
    thisurl="http://www.qiushibaike.com/hot/page/"+str(i+1)+'/'
    data=urllib.request.urlopen(thisurl).read().decode("utf-8","ignore")
    pat='<div class="content">.*?<span>(.*?)</span>.*?</div>'
    rst=re.compile(pat,re.S).findall(data)
    for j in range(0,len(rst)):
        print(rst[j])
        print("-------")
```

## request模块的使用


```python
import requests
import re
'''
请求方式:get、post、put…
参数：params、headers、proxies、cookies、data
'''
rsp=requests.get("https://www.hellobi.com/")
ck=requests.utils.dict_from_cookiejar(rsp.cookies)
title=re.compile("<title>(.*?)</title>",re.S).findall(rsp.text)
hd={"User-Agent":"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.221 Safari/537.36 SE 2.X MetaSr 1.0"}
px={"http":"http://127.0.0.1:8888",
    "https":"http://127.0.0.1:8888",}
rsp=requests.get("https://www.hellobi.com/",proxies=px,headers=hd,cookies=ck)
key={"wd":"韦玮",
     }
rsp=requests.get("http://www.baidu.com/s",headers=hd,cookies=ck,params=key)
title=re.compile("<title>(.*?)</title>",re.S).findall(rsp.text)

postdata={"name":"测试账号",
          "pass":"测试密码"}
rsp=requests.post("http://www.iqianyue.com/mypost/",data=postdata)

```

## BeautifulSoup的使用


```python
from bs4 import BeautifulSoup as bs
import urllib.request
data=urllib.request.urlopen("http://edu.iqianyue.com/").read().decode("utf-8","ignore")
bs1=bs(data)
#格式化输出
#print(bs1.prettify())
#获取标签：bs对象.标签名
bs1.title
#获取标签里面的文字:bs对象.标签名.string
bs1.title.string
#获取标签名：bs对象.标签名.name
bs1.title.name
#获取属性列表：bs对象.标签名.attrs
bs1.a.attrs
#获取某个属性对应的值:bs对象.标签名[属性名] 或者 bs对象.标签名.get(属性名)
bs1.a["class"]
bs1.a.get("class")
#提取所有某个节点的内容：bs对象.find_all('标签名') bs对象.find_all(['标签名1','标签名2，…,标签n'])
bs1.find_all('a')
bs1.find_all(['a','ul'])
#提取所有子节点:bs对象.标签.contents bs对象.标签.children
k1=bs1.ul.contents
k2=bs1.ul.children
allulc=[i for i in k2]
#更多信息可以阅读官方文档：http://beautifulsoup.readthedocs.io/zh_CN/latest/
```
